# Goroutine 调度器的进化史

## 概述

Go 调度器从 2009 年诞生至今，经历了多次重大升级。每次升级都针对性地解决了前一代的核心问题，体现了"渐进式改进"的设计哲学。

**核心目标的演进**：
```
2009-2012: 实现用户态调度
2012-2019: 追求极致性能和可扩展性
2020-至今: 平衡性能、公平性与可预测性
```

---

## 第一阶段：GM 模型（Go 1.0 之前，~2009-2011）

### 架构设计

```
所有 Goroutine
      ↓
全局队列（带全局锁）
      ↓
M1  M2  M3  M4 ... (OS 线程)
 ↓   ↓   ↓   ↓
竞争获取 Goroutine
```

**核心组件**：
- **G (Goroutine)**: 用户代码的执行单元
- **M (Machine)**: 操作系统线程

**数据结构**：
```go
// 简化示意
type g struct {
    stack       stack    // 栈信息
    sched       gobuf    // 保存的寄存器
    status      uint32   // 状态
}

type m struct {
    g0      *g    // 调度栈
    curg    *g    // 当前执行的 G
}

// 全局调度器
var sched struct {
    lock    mutex
    glist   *g    // 全局 G 队列
}
```

### 工作流程

```
1. 创建 Goroutine
   go func() { ... }
      ↓
   newproc() → 加锁 → 放入全局队列 → 解锁

2. M 获取 Goroutine
   schedule() → 加锁 → 从全局队列取 G → 解锁 → 执行 G

3. G 阻塞或完成
   → 加锁 → 更新全局队列 → 解锁 → schedule()
```

### 存在的问题

#### 问题 1：全局锁竞争严重

**场景**：
```
8 核机器，8 个 M 同时竞争全局锁：

M1: 尝试获取锁... 等待
M2: 尝试获取锁... 等待
M3: 获得锁 → 取出 G → 释放锁
M4: 尝试获取锁... 等待
M5: 尝试获取锁... 等待
...
```

**性能影响**：
- 锁竞争可能占用 40-50% 的 CPU 时间
- 随着核心数增加，竞争愈发严重
- 成为性能瓶颈

**基准测试**：
```
4 核机器：性能尚可
8 核机器：性能下降 30%
16 核机器：性能下降 60%（锁竞争主导）
```

#### 问题 2：缓存局部性差

**问题**：G 可能在不同 M 之间频繁迁移

```
G1 在 M1 上执行（CPU1）
   ↓ 阻塞
放回全局队列
   ↓
M2（CPU2）获取 G1
   ↓
CPU1 的缓存失效，CPU2 需要重新加载数据
```

**影响**：
- L1/L2 缓存失效
- 内存访问延迟增加
- 性能下降 20-30%

#### 问题 3：M 数量爆炸

**场景**：Goroutine 执行 syscall（如读文件）

```
M1 执行 G1 → G1 调用 read() → M1 阻塞（进入内核态）
   ↓
没有 M 可以执行其他 Goroutine
   ↓
创建新的 M2
   ↓
M2 执行 G2 → G2 调用 read() → M2 阻塞
   ↓
再创建 M3...
   ↓
M 数量持续增长
```

**后果**：
- M 创建开销大（系统调用、内核资源分配）
- 内存占用高（每个 M 有独立的栈）
- 上下文切换开销增加

#### 问题 4：扩展性差

**性能数据**（模拟）：
```
核心数   吞吐量   锁竞争占比
  2     100%      10%
  4     180%      25%
  8     280%      45%
 16     320%      70%
```

随着核心数增加，性能增长远低于线性。

### 优点

尽管存在诸多问题，GM 模型也有其优势：

1. **实现简单** - 代码易于理解和维护
2. **概念清晰** - G 映射到 M，直观
3. **证明可行** - 验证了用户态调度的可行性

### 为什么需要改进？

**根本矛盾**：
```
Go 的承诺：轻松创建百万 Goroutine
    ↓
全局锁：成为性能瓶颈
    ↓
必须：消除或减少全局锁竞争
```

---

## 第二阶段：GMP 模型（Go 1.1，2012 年 3 月）

### 革命性改进

**设计者**：Dmitry Vyukov（Google 工程师）

**设计文档**：《Scalable Go Scheduler Design》

**核心思想**：
> 通过引入 P（逻辑处理器），实现工作窃取调度，消除全局锁瓶颈

### 新架构

```
                全局 G 队列
                     ↓
         ┌───────────┼───────────┐
         ↓           ↓           ↓
    ┌────────┐  ┌────────┐  ┌────────┐
    │   P1   │  │   P2   │  │   P3   │
    │ 本地队列│  │ 本地队列│  │ 本地队列│
    │[G G G] │  │[G G G] │  │[G G G] │
    └────────┘  └────────┘  └────────┘
         ↓           ↓           ↓
       ┌─┴─┐      ┌─┴─┐      ┌─┴─┐
       │M1 │      │M2 │      │M3 │
       └───┘      └───┘      └───┘
```

### P（Processor）的引入

**P 是什么？**
- 逻辑处理器（Logical Processor）
- 执行 Go 代码所需的资源和上下文
- M 必须绑定 P 才能执行 G

**P 的数量**：
```go
// 默认等于 CPU 核心数
P 数量 = GOMAXPROCS
```

**P 的核心数据结构**：
```go
type p struct {
    status      uint32     // 状态：_Pidle, _Prunning, _Psyscall

    // 本地 G 队列（无锁）
    runqhead    uint32     // 队列头
    runqtail    uint32     // 队列尾
    runq        [256]guintptr  // 本地队列，最多 256 个
    runnext     guintptr   // 下一个优先执行的 G

    // 其他资源
    mcache      *mcache    // 内存分配缓存
    // ...
}
```

### 核心创新

#### 1. 本地队列（Local Run Queue）

**设计**：
- 每个 P 有独立的 G 队列
- 容量：256 个 Goroutine
- **无锁**访问（只有绑定的 M 访问）

**工作流程**：
```
创建新 Goroutine：
1. 尝试放入 P 的 runnext（优先执行）
2. 如果 runnext 已有，将其挤到本地队列
3. 如果本地队列满了（256个），将一半放入全局队列

获取 Goroutine：
1. 检查 runnext（优先）
2. 从本地队列头部取出
3. 如果本地队列空，使用 findRunnable()
```

**优势**：
```
无锁访问 → 极低延迟
本地队列 → 缓存友好
```

#### 2. 工作窃取（Work Stealing）

**算法**：
```
当 P 的本地队列为空时：

1. 检查全局队列（每 61 次调度检查一次，保证公平性）
   if (tick % 61 == 0) {
       从全局队列获取 G
   }

2. 检查 netpoller（网络就绪的 Goroutine）

3. 从其他 P 窃取：
   - 随机选择一个 P
   - 窃取其本地队列的一半

4. 再次检查全局队列

5. 如果都没有，进入休眠
```

**窃取示例**：
```
P1: [G1 G2 G3 G4 G5 G6 G7 G8]  ← 忙碌
P2: [G9]                       ← 即将空闲
P3: []                         ← 空闲

P3 从 P1 窃取：
  - 随机选中 P1
  - 从队列尾部窃取一半（4 个）

结果：
P1: [G1 G2 G3 G4]
P2: [G9]
P3: [G5 G6 G7 G8]  ← 获得工作
```

**实现细节**：
```go
// 简化版本
func runqsteal(_p_, p2 *p) *g {
    n := len(p2.runq) / 2  // 窃取一半

    // 从 p2 的队列尾部窃取
    for i := 0; i < n; i++ {
        g := p2.runq[tail]
        tail--
        _p_.runq[head] = g  // 放入自己的队列
        head++
    }

    return g
}
```

#### 3. P/M 分离机制

**问题场景**：Goroutine 执行 syscall

**传统做法**（GM 模型）：
```
G 调用 syscall → M 阻塞 → 创建新 M
```

**GMP 的方案**：
```
G 在 M1-P1 上执行 syscall
    ↓
entersyscall()：P1 状态改为 _Psyscall
    ↓
M1 进入内核态（阻塞）
    ↓
Sysmon 监控线程检测到 P1 长时间在 syscall 状态
    ↓
handoffp()：P1 从 M1 分离
    ↓
P1 寻找空闲的 M2（或创建新的）
    ↓
M2 绑定 P1，继续执行 P1 本地队列的其他 Goroutine
    ↓
（同时）M1 在后台等待 syscall 返回
    ↓
syscall 返回后，exitsyscall()：
  - M1 尝试重新获取 P1（如果仍空闲）
  - 否则尝试获取其他空闲 P
  - 都失败则将 G 放入全局队列，M1 休眠
```

**关键代码**：
```go
// runtime/proc.go
func entersyscall() {
    _g_ := getg()
    _g_.m.locks++

    // 保存当前状态
    save(sched)

    // 标记 P 进入 syscall 状态
    _g_.m.p.ptr().status = _Psyscall

    _g_.m.locks--
}

func exitsyscall() {
    _g_ := getg()

    // 尝试快速获取原来的 P
    if exitsyscallfast() {
        // 成功获取，继续执行
        return
    }

    // 失败，mcall 切换到 g0 执行 exitsyscall0
    mcall(exitsyscall0)
}
```

**优势**：
- P（调度上下文）不会被阻塞
- CPU 保持忙碌
- M 的数量增长可控

#### 4. GOMAXPROCS 的意义

**P 的数量决定了并行度**：
```go
// 示例
runtime.GOMAXPROCS(4)  // 设置 4 个 P

结果：
- 最多 4 个 Goroutine 同时执行
- 但可以有百万个 Goroutine 存在
```

**动态调整**：
```go
// 获取当前值
n := runtime.GOMAXPROCS(0)

// 设置新值
runtime.GOMAXPROCS(8)
```

### 性能提升

**Benchmark 数据**（Go 1.1 vs Go 1.0）：

```
测试场景              Go 1.0    Go 1.1    提升
──────────────────────────────────────────
创建 100 万 Goroutine   5.2s     0.8s     6.5x
Channel 通信           800ns    120ns     6.7x
并发计算（8核）        100%     620%      6.2x
网络服务器（8核）      100%     850%      8.5x
```

**扩展性测试**：
```
核心数   Go 1.0 性能   Go 1.1 性能   理论上限
  2        180%          195%         200%
  4        280%          380%         400%
  8        320%          750%         800%
 16        350%         1480%        1600%
```

### 解决的问题总结

| 问题 | GM 模型 | GMP 模型 | 改进效果 |
|------|---------|---------|---------|
| 全局锁竞争 | 严重 | 消除（本地队列） | ✅✅✅ |
| 缓存局部性 | 差 | 优秀（G 倾向同一 P） | ✅✅✅ |
| M 数量爆炸 | 严重 | 可控（P/M 分离） | ✅✅ |
| 负载均衡 | 无 | 工作窃取 | ✅✅✅ |
| 扩展性 | 差 | 线性扩展 | ✅✅✅ |

---

## 第三阶段：协作式抢占（Go 1.0 - 1.13，2012-2019）

### 抢占机制

**基本原理**：
在函数调用时检查是否需要抢占

**实现方式**：

#### 1. 编译器插入检查代码

```go
// 用户代码
func compute() {
    result := 0
    for i := 0; i < 1000000; i++ {
        result += i
    }
}

// 编译后（简化示意）
func compute() {
    if stackguard0 == stackPreempt {  // 检查抢占标记
        runtime.morestack()           // 触发抢占
    }

    result := 0
    for i := 0; i < 1000000; i++ {
        if stackguard0 == stackPreempt {  // 循环中的函数调用也会检查
            runtime.morestack()
        }
        result += i
    }
}
```

#### 2. 设置抢占标记

```go
// runtime/proc.go
func preemptone(_p_ *p) bool {
    gp := _p_.m.curg
    if gp == nil {
        return false
    }

    // 设置抢占标记
    gp.preempt = true
    gp.stackguard0 = stackPreempt  // 特殊值

    return true
}
```

#### 3. Sysmon 监控线程

```go
// 系统监控线程（不需要 P）
func sysmon() {
    for {
        usleep(delay)  // 动态延迟：20us ~ 10ms

        // 检查长时间运行的 P
        for _, _p_ := range allp {
            if _p_.status == _Prunning {
                if _p_.schedtick != _p_.syscalltick {
                    // P 正在运行，检查是否超时
                    if now - _p_.schedwhen > 10*ms {
                        preemptone(_p_)  // 设置抢占标记
                    }
                }
            }
        }
    }
}
```

### 抢占时机

**会触发抢占的场景**：

```
1. 函数调用
   func foo() {
       bar()  ← 这里检查抢占
   }

2. Channel 操作
   ch <- value  ← 可能触发调度
   <-ch         ← 可能触发调度

3. 系统调用
   syscall.Read()  ← 进入 syscall

4. 内存分配
   make([]int, 1000000)  ← 可能触发 GC，进而调度

5. 显式让出
   runtime.Gosched()  ← 主动让出 CPU
```

### 优点

1. **实现简单** - 与栈增长检查合并
2. **开销小** - 利用现有机制
3. **足够应对大多数场景** - 正常代码都有函数调用

### 存在的问题

#### 问题 1：紧密循环无法抢占

**典型场景**：
```go
// 死循环，永远占用 CPU
func busyLoop() {
    for {
        // 没有函数调用
        // 没有 channel 操作
        // 没有内存分配
        // → 永远不会被抢占！
    }
}

// 计算密集型循环
func compute() {
    sum := 0
    for i := 0; i < 10000000000; i++ {
        sum += i  // 纯计算，无函数调用
    }
    return sum
}
```

**实际影响**：
```go
func main() {
    runtime.GOMAXPROCS(2)

    go busyLoop()  // 占用一个 P

    go func() {
        fmt.Println("我想运行...")  // 可能永远无法执行
    }()

    time.Sleep(time.Hour)
}
```

#### 问题 2：GC 可能无法及时启动

**场景**：
```go
// Goroutine A：紧密循环
func work() {
    for {
        // 大量计算，无函数调用
    }
}

// 主 Goroutine
func main() {
    go work()

    // 触发 GC
    runtime.GC()  // 需要 STW（Stop The World）
                  // 但 work() 无法被停止！
                  // GC 只能等待...
}
```

**后果**：
- GC 延迟增加
- 内存占用升高
- 可能导致 OOM

#### 问题 3：不公平调度

**场景**：
```go
// G1：有函数调用，频繁被抢占
func g1() {
    for {
        foo()  // 函数调用，会被抢占
    }
}

// G2：紧密循环，从不被抢占
func g2() {
    for {
        // 纯计算，不会被抢占
    }
}

// 结果：G2 获得更多 CPU 时间
```

### 真实案例

**Go Issue #10958**（2015年）：
```go
// 用户报告的问题
func main() {
    runtime.GOMAXPROCS(1)

    go func() {
        for {
            // 密集计算
        }
    }()

    go func() {
        for {
            fmt.Println("Tick")  // 永远不会打印
            time.Sleep(time.Second)
        }
    }()

    select {}
}
```

**社区的 Workaround**：
```go
// 手动插入让步点
func compute() {
    for i := 0; i < 1000000000; i++ {
        if i % 100000 == 0 {
            runtime.Gosched()  // 手动让出
        }
        // 计算...
    }
}
```

### 为什么不早点解决？

**技术挑战**：
1. 需要在任意代码位置保存完整的寄存器状态
2. 需要安全点（Safe Point）机制
3. 需要处理各种边界情况（信号处理、栈扫描等）

**工程权衡**：
- 大多数实际代码都有函数调用
- 实现复杂度高
- 需要等待合适的时机

---

## 第四阶段：异步抢占（Go 1.14，2020 年 2 月）

### 重大突破

**设计目标**：
> 实现真正的抢占式调度，解决紧密循环问题

**设计文档**：
- Proposal: Non-cooperative goroutine preemption
- https://github.com/golang/proposal/blob/master/design/24543-non-cooperative-preemption.md

### 实现机制

#### 1. 基于信号的抢占

**核心思想**：
利用操作系统的信号机制，强制中断正在运行的 Goroutine

**选择的信号**：
```
Unix/Linux: SIGURG (Urgent condition on socket)
Windows: 使用 SuspendThread API
```

**为什么选择 SIGURG？**
- 不会被用户程序使用
- 不会干扰现有的信号处理
- 可以被安全处理

#### 2. 完整流程

```
┌─────────────────────────────────────────────────┐
│ 1. Sysmon 监控线程（每 10ms 检查一次）           │
│    检测到 G 运行超过 10ms                        │
└─────────────────┬───────────────────────────────┘
                  ↓
┌─────────────────────────────────────────────────┐
│ 2. preemptone(_p_)                              │
│    设置 G 的 preempt 标记                       │
│    调用 preemptM(G 所在的 M)                    │
└─────────────────┬───────────────────────────────┘
                  ↓
┌─────────────────────────────────────────────────┐
│ 3. signalM(M, sigPreempt)                       │
│    向 M 的底层线程发送 SIGURG 信号              │
└─────────────────┬───────────────────────────────┘
                  ↓
┌─────────────────────────────────────────────────┐
│ 4. M 收到信号，操作系统中断当前执行             │
│    跳转到信号处理函数 sighandler()              │
└─────────────────┬───────────────────────────────┘
                  ↓
┌─────────────────────────────────────────────────┐
│ 5. sighandler() → doSigPreempt()                │
│    保存 G 的完整上下文（所有寄存器）            │
│    调用 asyncPreempt()                          │
└─────────────────┬───────────────────────────────┘
                  ↓
┌─────────────────────────────────────────────────┐
│ 6. asyncPreempt()（汇编实现）                   │
│    切换到 g0 栈                                 │
│    调用 asyncPreempt2()                         │
└─────────────────┬───────────────────────────────┘
                  ↓
┌─────────────────────────────────────────────────┐
│ 7. asyncPreempt2()                              │
│    将 G 标记为 _Gpreempted                      │
│    调用 schedule() 选择下一个 G                 │
└─────────────────┬───────────────────────────────┘
                  ↓
┌─────────────────────────────────────────────────┐
│ 8. 被抢占的 G 重新进入就绪队列                  │
│    等待下次调度                                 │
└─────────────────────────────────────────────────┘
```

#### 3. 关键代码

**设置抢占**：
```go
// runtime/proc.go
func preemptone(_p_ *p) bool {
    mp := _p_.m.ptr()
    gp := mp.curg

    // 设置抢占标记
    gp.preempt = true
    gp.stackguard0 = stackPreempt

    // 请求异步抢占
    preemptM(mp)

    return true
}

func preemptM(mp *m) {
    if atomic.Cas(&mp.signalPending, 0, 1) {
        signalM(mp, sigPreempt)
    }
}
```

**发送信号**：
```go
// runtime/os_linux.go
func signalM(mp *m, sig int) {
    pthread_kill(pthread(mp.procid), sig)
}
```

**信号处理**（汇编 - runtime/preempt_amd64.s）：
```asm
TEXT ·asyncPreempt(SB),NOSPLIT|NOFRAME,$0-0
    // 保存所有寄存器
    PUSHQ BP
    PUSHQ BX
    PUSHQ R12
    PUSHQ R13
    PUSHQ R14
    PUSHQ R15

    // 保存浮点寄存器
    ADJSP $288
    MOVUPS X0, 0(SP)
    MOVUPS X1, 16(SP)
    // ... 保存 X0-X15

    // 切换到 g0 执行
    CALL ·asyncPreempt2(SB)

    // 恢复所有寄存器
    MOVUPS 0(SP), X0
    MOVUPS 16(SP), X1
    // ...

    POPQ R15
    POPQ R14
    POPQ R13
    POPQ R12
    POPQ BX
    POPQ BP

    RET
```

### 抢占条件

**何时触发抢占？**

```go
// runtime/proc.go - sysmon()
func sysmon() {
    for {
        // ...

        // 检查所有 P
        for _, _p_ := range allp {
            pd := &_p_.sysmontick
            s := _p_.status

            if s == _Prunning || s == _Psyscall {
                // 运行时间超过 10ms
                t := now - pd.schedwhen
                if t > 10*1000*1000 {  // 10ms
                    preemptone(_p_)
                }
            }
        }
    }
}
```

**其他触发场景**：
1. **GC 需要 STW**：强制抢占所有 G
2. **栈扫描**：GC 标记阶段需要扫描 G 的栈
3. **调试**：调试器请求暂停

### 安全性保证

#### 1. 安全点（Safe Point）

**不是所有时刻都能安全抢占**：
```go
// 不安全的时刻
func foo() {
    lock(&mutex)
    // ← 这里被抢占可能导致死锁
    // ...
    unlock(&mutex)
}
```

**解决方案**：
```go
type m struct {
    locks      int32  // 持有的锁数量
    preemptoff string // 禁止抢占的原因
}

// 关键区域禁止抢占
func lockOSThread() {
    _g_ := getg()
    _g_.m.lockedg = _g_
    _g_.lockedm = _g_.m
}
```

#### 2. 栈扫描保护

**GC 扫描栈时需要一致的视图**：
```go
// GC 扫描前，必须先暂停 G
func scanstack(gp *g) {
    if gp.preemptStop {
        // 已经被抢占，可以安全扫描
    } else {
        // 请求抢占并等待
        gp.preemptStop = true
        preemptM(gp.m)
        // 等待 G 暂停...
    }

    // 扫描栈
    scanblock(gp.stack)
}
```

### 性能影响

**开销分析**：
```
Sysmon 检查周期：10ms
单次检查开销：~50μs（检查所有 P）
抢占信号处理：~1-2μs（保存/恢复寄存器）

总开销：< 1% CPU 时间
```

**Benchmark 对比**（Go 1.13 vs Go 1.14）：
```
场景                  Go 1.13   Go 1.14   差异
────────────────────────────────────────────
正常调度              100%      100%      0%
紧密循环（单 G）      100%      100%      0%
紧密循环 + 其他 G     阻塞      正常      ✅
GC STW 延迟          0-100ms   0-10ms    ✅
公平性               差        优秀      ✅
```

### 解决的问题

#### ✅ 紧密循环可以被抢占

**之前**：
```go
func main() {
    runtime.GOMAXPROCS(1)

    go func() {
        for {
            // 占用 CPU，其他 G 饿死
        }
    }()

    go func() {
        fmt.Println("Never printed")  // Go 1.13: 永不执行
    }()
}
```

**现在（Go 1.14+）**：
```
10ms 后被抢占 → 第二个 G 获得执行机会 ✅
```

#### ✅ GC 更及时

**STW 延迟对比**：
```
Go 1.13:
- 最好情况：<1ms
- 最坏情况：>100ms（等待紧密循环的 G）

Go 1.14+:
- 最好情况：<1ms
- 最坏情况：~10ms（强制抢占）
```

#### ✅ 公平性提升

**时间片保证**：
- 每个 G 运行不超过 10ms
- 所有 G 都有机会执行
- 避免饥饿

### 实际案例

**案例 1：HTTP 服务器**
```go
// Go 1.13: 可能因某个请求的紧密计算导致其他请求超时
// Go 1.14: 保证所有请求都能得到响应

func handler(w http.ResponseWriter, r *http.Request) {
    // 密集计算
    for i := 0; i < 1000000000; i++ {
        // Go 1.14 会在 10ms 后被抢占
        // 其他请求不会被阻塞
    }
}
```

**案例 2：实时系统**
```go
// Go 1.14+ 更适合软实时场景
func main() {
    go backgroundWork()  // 密集计算

    go func() {
        ticker := time.NewTicker(10 * time.Millisecond)
        for range ticker.C {
            // Go 1.14 保证每 10ms 能执行一次
            collectMetrics()
        }
    }()
}
```

---

## 第五阶段：持续优化（Go 1.14 至今）

### Go 1.15-1.16（2020-2021）

#### 优化 1：改进抢占时机判断

**问题**：不是所有长时间运行的 G 都应该被抢占

**改进**：
```go
// 增加启发式判断
func preemptone(_p_ *p) bool {
    gp := _p_.m.curg

    // 不抢占系统 Goroutine
    if gp.isSystemGoroutine() {
        return false
    }

    // 不抢占正在执行 GC 的 G
    if gp.gcAssistBytes != 0 {
        return false
    }

    // 正常抢占
    return doPreempt(gp)
}
```

#### 优化 2：减少 Sysmon 开销

**Go 1.15 改进**：
```go
// 动态调整检查频率
func sysmon() {
    delay := uint32(0)

    for {
        if idle == 0 {
            delay = 20  // 有活跃 P，频繁检查（20μs）
        } else {
            delay *= 2  // 空闲时降低频率
            if delay > 10000 {  // 最多 10ms
                delay = 10000
            }
        }

        usleep(delay)
        // ...
    }
}
```

### Go 1.17（2021 年 8 月）

#### 优化：基于寄存器的调用约定

**之前**：基于栈的参数传递
```asm
// 函数调用（Go 1.16）
CALL foo(SB)
  - 参数通过栈传递
  - 返回值通过栈传递
  - 需要操作栈指针
```

**改进**：利用寄存器传递
```asm
// 函数调用（Go 1.17+）
CALL foo(SB)
  - 参数通过寄存器传递（RAX, RBX, ...）
  - 返回值通过寄存器传递
  - 更少的内存访问
```

**性能提升**：
```
函数调用密集型代码：5-10% 性能提升
调度切换：~15% 更快（寄存器保存/恢复更简单）
```

### Go 1.18（2022 年 3 月）

#### 泛型对调度的影响

**新挑战**：泛型实例化导致更多函数

**优化**：
```go
// 泛型函数的调度优化
func Sort[T any](slice []T, less func(T, T) bool) {
    // 编译器优化：
    // - 常用类型（int, string）特化
    // - 减少接口调用开销
}
```

### Go 1.19（2022 年 8 月）

#### 优化 1：软内存限制

**与调度器的交互**：
```go
// runtime/mgc.go
func gcStart(mode gcMode) {
    // 内存压力大时，调整调度策略
    if memstats.heapAlloc > softLimit {
        // 更频繁地触发 GC
        // 更积极地抢占分配内存的 G
    }
}
```

#### 优化 2：改进 Work Stealing

**Go 1.19 改进**：
```go
// 更智能的窃取策略
func stealWork(now int64) *g {
    // 1. 优先从 CPU 亲和的 P 窃取（缓存友好）
    // 2. 窃取时考虑 P 的负载
    // 3. 避免过度窃取
}
```

### Go 1.20（2023 年 2 月）

#### 优化：PGO（Profile-Guided Optimization）

**与调度的结合**：
```
运行时收集 profile 数据
    ↓
识别热点 Goroutine
    ↓
优化调度决策：
  - 热点 G 优先调度
  - 减少热点 G 的抢占
```

### Go 1.21-1.22（2023-2024）

#### 改进调度器观测性

**新工具**：
```bash
# 1. 更详细的调度 trace
go tool trace -http=:8080 trace.out

# 2. 运行时指标
GODEBUG=schedtrace=1000,scheddetail=1 ./program
```

**输出示例**：
```
SCHED 1000ms: gomaxprocs=8 idleprocs=0 threads=10 spinningthreads=0
  idlethreads=2 runqueue=0 [0 0 0 0 0 0 0 0]

解释：
- gomaxprocs=8: 8 个 P
- idleprocs=0: 所有 P 都在工作
- threads=10: 10 个 M
- runqueue=0: 全局队列为空
- [0 0 0 0 0 0 0 0]: 各 P 的本地队列长度
```

### Go 1.23（2024）- 最新改进

#### 优化：更好的 Goroutine 局部性

**改进**：
```go
// 相关的 Goroutine 倾向于在同一个 P 上执行
func spawn() {
    parent := getg()

    go func() {
        child := getg()
        // 尝试将 child 调度到 parent 所在的 P
        // 提升缓存命中率
    }()
}
```

---

## 调度器进化的设计哲学

### 核心原则

#### 1. 渐进式改进（Incremental Improvement）

```
不追求一次性完美，而是持续优化：
GM → GMP → 异步抢占 → 持续优化
每一步都解决最紧迫的问题
```

#### 2. 向后兼容（Backward Compatibility）

```
每次升级都保持 API 稳定：
- 用户代码无需修改
- 行为变化在合理范围内
- 性能只会更好，不会更差
```

#### 3. 测量驱动（Measurement-Driven）

```
基于真实数据做决策：
- 大量 Benchmark
- 生产环境反馈
- 社区 Issue 跟踪
```

### 权衡与取舍

| 维度 | GM 模型 | GMP 模型 | 异步抢占 |
|------|---------|---------|---------|
| **性能** | 低 | 高 | 高 |
| **公平性** | 中 | 中 | 高 |
| **复杂度** | 低 | 中 | 高 |
| **可预测性** | 中 | 中 | 高 |
| **实现成本** | 低 | 中 | 高 |

**演进趋势**：
```
初期：简单 > 性能
中期：性能 > 简单
成熟：性能 + 公平性 + 可预测性
```

---

## 与其他语言的对比

### Go vs Java（JVM）

| 特性 | Go 调度器 | Java 线程 |
|------|----------|-----------|
| 调度层次 | 用户态（Runtime） | 操作系统 |
| 并发单元 | Goroutine（2KB） | Thread（1-2MB） |
| 调度算法 | Work Stealing | OS 调度器 |
| 抢占方式 | 信号抢占（Go 1.14+） | OS 时间片 |
| 数量限制 | 百万级 | 千级 |

**Java 虚拟线程（Loom，JDK 19+）**：
```
受 Go 启发，引入轻量级线程：
- 类似 Goroutine 的概念
- 用户态调度
- 但调度器设计不同
```

### Go vs Rust（Tokio）

| 特性 | Go 调度器 | Tokio 运行时 |
|------|----------|-------------|
| 调度模型 | 抢占式 | 协作式 |
| 并发单元 | Goroutine | Task/Future |
| 调度器 | Runtime 内置 | 库实现 |
| 抢占 | 支持（异步） | 需要手动 await |
| 使用复杂度 | 简单（go func） | 复杂（async/await） |

### Go vs Erlang/Elixir（BEAM VM）

| 特性 | Go 调度器 | BEAM 调度器 |
|------|----------|------------|
| 调度单元 | Goroutine | Process |
| 抢占方式 | 信号（10ms） | 归约计数（2000 次） |
| 隔离性 | 共享内存 | 完全隔离 |
| 消息传递 | Channel | Mailbox |
| 容错 | Panic/Recover | Let it crash |

**Go 的优势**：
- 性能更高（共享内存）
- 更低的内存开销

**BEAM 的优势**：
- 更强的隔离性
- 更成熟的容错机制

---

## 未来展望

### 可能的改进方向

#### 1. 更细粒度的调度控制

**可能的特性**：
```go
// 用户可指定 Goroutine 优先级
go:priority(high) func criticalTask() {
    // 高优先级任务
}

// CPU 亲和性
go:affinity(core=2) func cpuBoundTask() {
    // 绑定到特定 CPU 核心
}
```

#### 2. NUMA 感知调度

**优化方向**：
```
多插槽服务器（NUMA 架构）：
├─ CPU 0-7  + Memory Bank 0
└─ CPU 8-15 + Memory Bank 1

优化：
- P 绑定到 NUMA 节点
- G 尽量访问本地内存
- 减少跨节点内存访问
```

#### 3. 更智能的工作窃取

**可能的改进**：
```go
// 基于机器学习的窃取策略
func stealWork() *g {
    // 考虑：
    // - 历史执行时间
    // - 缓存亲和性
    // - 内存访问模式
    // - 功耗优化
}
```

#### 4. 异构计算支持

**场景**：
```
CPU + GPU 混合调度：
├─ 计算密集型 G → GPU
├─ I/O 密集型 G → CPU
└─ 调度器自动选择最优硬件
```

### 社区讨论的提案

#### Proposal 1：用户态抢占点标记

```go
// 允许用户标记安全抢占点
func longComputation() {
    for i := 0; i < 1000000000; i++ {
        // 计算...

        if i % 100000 == 0 {
            runtime.PreemptionPoint()  // 提示：这里可以抢占
        }
    }
}
```

#### Proposal 2：调度器插件化

```go
// 自定义调度策略
type Scheduler interface {
    Schedule(gq *gQueue) *g
    Preempt(g *g) bool
}

runtime.RegisterScheduler(MyScheduler{})
```

### 性能极限

**理论上限**：
```
单次调度开销：~200ns（目前已接近）
抢占延迟：~10ms（Go 1.14 已实现）

进一步优化空间有限，重点转向：
- 特定场景优化
- 可观测性
- 用户控制能力
```

---

## 总结

### 进化历程一览

```
2009   2012              2020        现在    未来
 │      │                 │           │       │
 ├─ GM ─┼─── GMP ─────────┼─ 异步抢占 ─┼─ 持续优化 ─→ ?
 │      │                 │           │
 │      │                 │           └─ PGO、观测性
 │      │                 └─ 信号抢占、公平性
 │      └─ P、本地队列、Work Stealing
 └─ 用户态调度、协作式抢占
```

### 核心问题与解决方案

| 阶段 | 核心问题 | 解决方案 | 关键技术 |
|------|---------|---------|---------|
| **GM** | 全局锁竞争 | 引入 P | 本地队列 |
| **GMP** | 负载不均 | 工作窃取 | 随机窃取 |
| **GMP** | Syscall 阻塞 | P/M 分离 | Handoff |
| **协作式** | 紧密循环 | 异步抢占 | 信号机制 |
| **现代** | 可观测性 | 工具链 | Trace/Profile |

### 设计亮点

1. **分层设计**：
   ```
   用户代码 (Goroutine)
       ↓
   Runtime 调度器 (GMP)
       ↓
   操作系统 (线程)
       ↓
   硬件 (CPU)
   ```

2. **局部优先，全局兜底**：
   ```
   本地队列（快速、无锁）
       ↓ 失败
   全局队列（中等、有锁）
       ↓ 失败
   工作窃取（慢速、平衡）
   ```

3. **渐进式改进**：
   - 每个版本解决最紧迫的问题
   - 保持向后兼容
   - 基于实际测量

### 关键启示

**对语言设计者**：
- 性能优化是持续的过程
- 真实世界反馈比理论重要
- 兼容性是长期成功的关键

**对 Go 使用者**：
- 理解调度器有助于写出高效代码
- 大多数情况下相信调度器
- 必要时可以手动优化（runtime.GOMAXPROCS、runtime.LockOSThread）

**对系统设计者**：
- 用户态调度是高并发的关键
- Work Stealing 是负载均衡的利器
- 抢占式调度对公平性至关重要

---

## 参考资料

### 官方文档
- [The Go Memory Model](https://go.dev/ref/mem)
- [Go Scheduler Design Doc](https://go.dev/s/go11sched)
- [Non-cooperative Goroutine Preemption](https://github.com/golang/proposal/blob/master/design/24543-non-cooperative-preemption.md)

### 经典文章
- "Scalable Go Scheduler Design Doc" - Dmitry Vyukov (2012)
- "Go Preemptive Scheduler Design Doc" - Austin Clements (2019)
- "The Go scheduler" - Morsing (2013)

### 源码位置
```
核心文件：
- runtime/proc.go       - 调度核心逻辑
- runtime/runtime2.go   - 数据结构定义
- runtime/preempt.go    - 抢占实现
- runtime/preempt_*.s   - 抢占汇编代码
- runtime/mgc.go        - GC 与调度的交互

历史版本对比：
- Go 1.0:  git.io/v1.0
- Go 1.1:  git.io/v1.1  (GMP 引入)
- Go 1.14: git.io/v1.14 (异步抢占)
```

### 社区资源
- GopherCon 演讲：搜索 "Go Scheduler"
- Go Runtime 源码阅读小组
- Golang GitHub Issues（搜索 "scheduler"）

---

**最后的话**：

Go 调度器的进化史展示了**工程实践的智慧**：
- 没有一步到位的完美方案
- 基于真实问题持续改进
- 在性能、复杂度、兼容性之间找到平衡

这也是 Go 语言成功的缩影：**简单、高效、务实**。
